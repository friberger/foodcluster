{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import random\n",
    "import urllib.request\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[364, 13, 1395] [['Tomater gröna syltade' '421']\n",
      " ['Lättmargarin m växtsterol 7,5% fett 35% berik typ Becel proactiv' '18']\n",
      " ['Smördeg gräddad' '1640']]\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('livs.db')  # Create db and establish connection\n",
    "conn.row_factory = sqlite3.Row\n",
    "curs = conn.cursor()\n",
    "result = []\n",
    "rows = curs.execute('select Livsmedelsnamn, Livsmedelsnummer from livs limit 2000')\n",
    "for row in rows:\n",
    "        result.append(row)\n",
    "\n",
    "#rows = np.array(result)\n",
    "#rows = rows[970:]\n",
    "samples = random.sample(range(0, 1999), 3)\n",
    "sampledFoodItems = []\n",
    "#print (type(sampledFoodItems))\n",
    "\n",
    "for index in samples:\n",
    "    sampledFoodItems.append(result[index])\n",
    "\n",
    "sampledFoodItems = np.array(sampledFoodItems)\n",
    "print (samples, sampledFoodItems)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_page(url): #urllib library for Extracting web pages\n",
    "    try:\n",
    "        headers = {}\n",
    "        headers['User-Agent'] = \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\"\n",
    "        req = urllib.request.Request(url, headers = headers)\n",
    "        resp = urllib.request.urlopen(req)\n",
    "        respData = str(resp.read())\n",
    "        return respData\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding 'Next Image' from the given raw page\n",
    "def _images_get_next_item(s):\n",
    "    print (type(s))\n",
    "    start_line = s.find('rg_di')\n",
    "    if start_line == -1:    #If no links are found then give an error!\n",
    "        end_quote = 0\n",
    "        link = \"no_links\"\n",
    "        return link, end_quote\n",
    "    else:\n",
    "        start_line = s.find('\"class=\"rg_meta\"')\n",
    "        start_content = s.find('\"ou\"',start_line+1)\n",
    "        end_content = s.find(',\"ow\"',start_content+1)\n",
    "        content_raw = str(s[start_content+6:end_content-1])\n",
    "        return content_raw, end_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _images_get_all_items(page):\n",
    "    items = []\n",
    "    while True:\n",
    "        item, end_content = _images_get_next_item(page)\n",
    "        if item == \"no_links\":\n",
    "            break\n",
    "        else:\n",
    "            items.append(item)      #Append all the links in the list named 'Links'\n",
    "            time.sleep(0.1)        #Timer could be used to slow down the request for image downloads\n",
    "            page = page[end_content:]\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Tomater%20gröna%20syltade' '421']\n",
      " ['Lättmargarin%20m%20växtsterol%207,5%%20fett%2035%%20berik%20typ%' '18']\n",
      " ['Smördeg%20gräddad' '1640']]\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()   #start the timer\n",
    "keywords = sampledFoodItems\n",
    "#Prepare keywords\n",
    "for i, item in enumerate(keywords):\n",
    "    keywords[i][0]=keywords[i][0].replace(' ','%20')\n",
    "print (keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item no.: 0 --> Item name = Tomater%20gröna%20syltade\n",
      "Evaluating...\n",
      "['Tomater%20gröna%20syltade' '421']\n",
      "https://www.google.se/search?q=Tomater%20gröna%20syltade&espv=2&biw=1366&bih=667&site=webhp&source=lnms&tbm=isch&sa=X&ei=XosDVaCXD8TasATItgE&ved=0CAcQ_AUoAg\n",
      "Item no.: 1 --> Item name = Lättmargarin%20m%20växtsterol%207,5%%20fett%2035%%20berik%20typ%\n",
      "Evaluating...\n",
      "['Lättmargarin%20m%20växtsterol%207,5%%20fett%2035%%20berik%20typ%' '18']\n",
      "https://www.google.se/search?q=Lättmargarin%20m%20växtsterol%207,5%%20fett%2035%%20berik%20typ%&espv=2&biw=1366&bih=667&site=webhp&source=lnms&tbm=isch&sa=X&ei=XosDVaCXD8TasATItgE&ved=0CAcQ_AUoAg\n",
      "Item no.: 2 --> Item name = Smördeg%20gräddad\n",
      "Evaluating...\n",
      "['Smördeg%20gräddad' '1640']\n",
      "https://www.google.se/search?q=Smördeg%20gräddad&espv=2&biw=1366&bih=667&site=webhp&source=lnms&tbm=isch&sa=X&ei=XosDVaCXD8TasATItgE&ved=0CAcQ_AUoAg\n"
     ]
    }
   ],
   "source": [
    "    #Download Image Links\n",
    "    for index, item in enumerate(keywords):\n",
    "        iteration = \"Item no.: \" + str(index) + \" -->\" + \" Item name = \" + str(item[0])\n",
    "        print (iteration)\n",
    "        print (\"Evaluating...\")\n",
    "        print(item)\n",
    "\n",
    "         #make a search keyword  directory\n",
    "        try:\n",
    "            os.makedirs(item[1])\n",
    "        except OSError as e:\n",
    "            if e.errno != 17:\n",
    "                raise\n",
    "            # time.sleep might help here\n",
    "            pass\n",
    "\n",
    "\n",
    "        url = 'https://www.google.se/search?q=' + item[0] +'&espv=2&biw=1366&bih=667&site=webhp&source=lnms&tbm=isch&sa=X&ei=XosDVaCXD8TasATItgE&ved=0CAcQ_AUoAg'\n",
    "        print(url)\n",
    "        #raw_html =  download_page(url)\n",
    "        #print(raw_html)\n",
    "        \n",
    "        #time.sleep(0.1)\n",
    "        #imageLinks = (_images_get_all_items(raw_html)) # + items\n",
    "\n",
    "        #print (\"Image Links = \"+str(imageLinks))\n",
    "        #print (\"Total Image Links = \"+str(len(imageLinks)))\n",
    "        #print (\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downloadImages(keyword):\n",
    "    k=0\n",
    "    numberOfImages = 5 # Max is len(items)\n",
    "    errorCount=0\n",
    "    while(k<numberOfImages):\n",
    "        import urllib.request\n",
    "        import urllib.error\n",
    "\n",
    "        try:\n",
    "            req = urllib.request.Request(items[k], headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"})\n",
    "            response = urllib.request.urlopen(req,None,15)\n",
    "            output_file = open(keyword[1]+\"/\"+str(k+1)+\".jpg\",'wb')\n",
    "\n",
    "            data = response.read()\n",
    "            output_file.write(data)\n",
    "            response.close();\n",
    "\n",
    "            print(\"completed ====> \"+str(k+1))\n",
    "\n",
    "            k=k+1;\n",
    "\n",
    "        except urllib.error.HTTPError as e:  #If there is any HTTPError\n",
    "\n",
    "            errorCount+=1\n",
    "            print(\"HTTPError\"+str(k))\n",
    "            k=k+1;\n",
    "        except urllib.error.URLError as e:\n",
    "\n",
    "            errorCount+=1\n",
    "            print(\"URLError \"+str(k))\n",
    "            k=k+1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 357.4313762187958 Seconds\n",
      "Starting Download...\n",
      "['Tomater%20gröna%20syltade' '421']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'items' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-77fda4ccd122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# IN this saving process we are just skipping the URL if there is any error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdownloadImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-0ab4c08a55b5>\u001b[0m in \u001b[0;36mdownloadImages\u001b[0;34m(keyword)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'items' is not defined"
     ]
    }
   ],
   "source": [
    "#This allows you to write all the links into a test file. This text file will be created in the same directory as your code. You can comment out the below 3 lines to stop writing the output to the text file.\n",
    "#info = open('output.txt', 'a')        #Open the text file called database.txt\n",
    "#info.write(str(i) + ': ' + str(search_keyword[i-1]) + \": \" + str(items) + \"\\n\\n\\n\")         #Write the title of the page\n",
    "#info.close()                            #Close the file\n",
    "\n",
    "t1 = time.time()    #stop the timer\n",
    "total_time = t1-t0   #Calculating the total time required to crawl, find and download all the links of 60,000 images\n",
    "print(\"Total time taken: \"+str(total_time)+\" Seconds\")\n",
    "print (\"Starting Download...\")\n",
    "\n",
    "## To save imges to the same directory\n",
    "# IN this saving process we are just skipping the URL if there is any error\n",
    "print(keywords[0])\n",
    "downloadImages(keywords[0])\n",
    "i = i+1\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Everything downloaded!\")\n",
    "print(\"\\n\"+str(errorCount)+\" ----> total Errors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
